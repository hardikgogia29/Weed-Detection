{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Dataset and creating dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shutil import copy2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import supervision as sv\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from torchvision.ops import box_iou\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE THE FILE PATHS WITH YOUR RESPECTIVE FILE PATH\n",
    "\n",
    "labeled_images_path = \"/kaggle/input/weed-detection-dataset/labeled/images\"\n",
    "labeled_annotations_path = \"/kaggle/input/weed-detection-dataset/labeled/annotations\"\n",
    "test_images_path = \"/kaggle/input/weed-detection-dataset/test/images\"\n",
    "test_annotations_path = \"/kaggle/input/weed-detection-dataset/test/annotations\"\n",
    "unlabeled_images_path = \"/kaggle/input/weed-detection-dataset/unlabeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:06.655243Z",
     "iopub.status.busy": "2025-02-05T16:31:06.654954Z",
     "iopub.status.idle": "2025-02-05T16:31:07.136866Z",
     "shell.execute_reply": "2025-02-05T16:31:07.136157Z",
     "shell.execute_reply.started": "2025-02-05T16:31:06.655219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>x_centre</th>\n",
       "      <th>y_centre</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.362305</td>\n",
       "      <td>0.416992</td>\n",
       "      <td>0.669922</td>\n",
       "      <td>0.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>0.487305</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.904297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.411133</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.341797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.445312</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.220703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.337891</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.574219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.499023</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>0.748047</td>\n",
       "      <td>0.603516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.673828</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.371094</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524414</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477539</td>\n",
       "      <td>0.533203</td>\n",
       "      <td>0.451172</td>\n",
       "      <td>0.738281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 image  label  x_centre  \\\n",
       "0    /kaggle/input/weed-detection-dataset/labeled/i...      1  0.362305   \n",
       "1    /kaggle/input/weed-detection-dataset/labeled/i...      0  0.511719   \n",
       "2    /kaggle/input/weed-detection-dataset/labeled/i...      1  0.437500   \n",
       "3    /kaggle/input/weed-detection-dataset/labeled/i...      1  0.554688   \n",
       "4    /kaggle/input/weed-detection-dataset/labeled/i...      0  0.235352   \n",
       "..                                                 ...    ...       ...   \n",
       "195  /kaggle/input/weed-detection-dataset/labeled/i...      0  0.335938   \n",
       "196  /kaggle/input/weed-detection-dataset/labeled/i...      1  0.499023   \n",
       "197  /kaggle/input/weed-detection-dataset/labeled/i...      1  0.673828   \n",
       "198  /kaggle/input/weed-detection-dataset/labeled/i...      0  0.524414   \n",
       "199  /kaggle/input/weed-detection-dataset/labeled/i...      1  0.477539   \n",
       "\n",
       "     y_centre     width    height  \n",
       "0    0.416992  0.669922  0.759766  \n",
       "1    0.487305  0.953125  0.904297  \n",
       "2    0.411133  0.375000  0.341797  \n",
       "3    0.445312  0.554688  0.351562  \n",
       "4    0.213867  0.087891  0.220703  \n",
       "..        ...       ...       ...  \n",
       "195  0.337891  0.632812  0.574219  \n",
       "196  0.348633  0.748047  0.603516  \n",
       "197  0.279297  0.371094  0.453125  \n",
       "198  0.531250  0.767578  0.855469  \n",
       "199  0.533203  0.451172  0.738281  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_dataframe(images_path, annotations_path):\n",
    "    data = []\n",
    "    for image_file in os.listdir(images_path):\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        annotation_file = image_file.replace(\".jpg\", \".txt\")\n",
    "        annotation_path = os.path.join(annotations_path, annotation_file)\n",
    "        \n",
    "        if os.path.exists(annotation_path):\n",
    "            with open(annotation_path, \"r\") as f:\n",
    "                line = f.readline().strip().split()  # Read first line\n",
    "                if len(line) == 5:  # Ensure valid YOLO format\n",
    "                    label, x_centre, y_centre, width, height = map(float, line)\n",
    "                    data.append({\"image\": image_path, \"label\": int(label), \n",
    "                                 \"x_centre\": x_centre, \"y_centre\": y_centre, \n",
    "                                 \"width\": width, \"height\": height})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = create_dataframe(labeled_images_path, labeled_annotations_path)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:13.308343Z",
     "iopub.status.busy": "2025-02-05T16:31:13.308062Z",
     "iopub.status.idle": "2025-02-05T16:31:13.435164Z",
     "shell.execute_reply": "2025-02-05T16:31:13.434524Z",
     "shell.execute_reply.started": "2025-02-05T16:31:13.308320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = create_dataframe(test_images_path, test_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:13.753539Z",
     "iopub.status.busy": "2025-02-05T16:31:13.753301Z",
     "iopub.status.idle": "2025-02-05T16:31:17.138794Z",
     "shell.execute_reply": "2025-02-05T16:31:17.137687Z",
     "shell.execute_reply.started": "2025-02-05T16:31:13.753518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.71)\n",
      "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.3.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics supervision numpy opencv-python torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:21.829354Z",
     "iopub.status.busy": "2025-02-05T16:31:21.829060Z",
     "iopub.status.idle": "2025-02-05T16:31:21.835313Z",
     "shell.execute_reply": "2025-02-05T16:31:21.834424Z",
     "shell.execute_reply.started": "2025-02-05T16:31:21.829323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset folders created.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset\"\n",
    "os.makedirs(f\"{dataset_path}/images/train\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/images/val\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/labels/train\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/labels/val\", exist_ok=True)\n",
    "\n",
    "print(\"âœ… Dataset folders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:22.197760Z",
     "iopub.status.busy": "2025-02-05T16:31:22.197486Z",
     "iopub.status.idle": "2025-02-05T16:31:22.650169Z",
     "shell.execute_reply": "2025-02-05T16:31:22.649328Z",
     "shell.execute_reply.started": "2025-02-05T16:31:22.197736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Images copied to dataset folders.\n"
     ]
    }
   ],
   "source": [
    "def move_images(df, img_folder):\n",
    "    for img_path in df[\"image\"]:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        new_path = os.path.join(img_folder, img_name)\n",
    "        shutil.copy(img_path, new_path)\n",
    "\n",
    "# âœ… Move training images\n",
    "move_images(df, \"dataset/images/train\")\n",
    "\n",
    "# âœ… Move validation images (from test_df)\n",
    "move_images(test_df, \"dataset/images/val\")\n",
    "\n",
    "print(\"âœ… Images copied to dataset folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:24.074092Z",
     "iopub.status.busy": "2025-02-05T16:31:24.073781Z",
     "iopub.status.idle": "2025-02-05T16:31:24.128319Z",
     "shell.execute_reply": "2025-02-05T16:31:24.127493Z",
     "shell.execute_reply.started": "2025-02-05T16:31:24.074066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… YOLO labels saved.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_yolo(df, label_folder):\n",
    "    for _, row in df.iterrows():\n",
    "        img_name = os.path.basename(row[\"image\"]).replace(\".jpg\", \".txt\")  \n",
    "        label_file = os.path.join(label_folder, img_name)\n",
    "\n",
    "        with open(label_file, \"w\") as f:\n",
    "            f.write(f\"{row['label']} {row['x_centre']} {row['y_centre']} {row['width']} {row['height']}\\n\")\n",
    "\n",
    "# âœ… Create labels for training and validation\n",
    "convert_to_yolo(df, \"dataset/labels/train\")\n",
    "convert_to_yolo(test_df, \"dataset/labels/val\")\n",
    "\n",
    "print(\"âœ… YOLO labels saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:25.373800Z",
     "iopub.status.busy": "2025-02-05T16:31:25.373515Z",
     "iopub.status.idle": "2025-02-05T16:31:25.379206Z",
     "shell.execute_reply": "2025-02-05T16:31:25.378549Z",
     "shell.execute_reply.started": "2025-02-05T16:31:25.373775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dataset.yaml created.\n"
     ]
    }
   ],
   "source": [
    "dataset_abs_path = os.path.abspath(\"dataset\")\n",
    "\n",
    "yaml_content = f\"\"\"\n",
    "path: {dataset_abs_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: 2\n",
    "names: [\"weed\", \"non-weed\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dataset/dataset.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"âœ… dataset.yaml created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training only on labeled dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:31:26.802817Z",
     "iopub.status.busy": "2025-02-05T16:31:26.802539Z",
     "iopub.status.idle": "2025-02-05T16:32:42.878263Z",
     "shell.execute_reply": "2025-02-05T16:32:42.877377Z",
     "shell.execute_reply.started": "2025-02-05T16:31:26.802794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.71 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset/dataset.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/labels/train.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/labels/val.cache... 50 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20       2.3G      1.606      3.316      1.989         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  5.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0032       0.96      0.317      0.174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      2.19G      1.303      2.489      1.722         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50    0.00357      0.981      0.449      0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      2.18G       1.23      2.136      1.623         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.643      0.274      0.537      0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      2.18G      1.219      2.003      1.625         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.711      0.527      0.635      0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      2.18G      1.248      1.853      1.664         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.621      0.771      0.717      0.321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      2.18G      1.254      1.921      1.662         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.593      0.546      0.532      0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      2.18G      1.312      1.933      1.712         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.491      0.627      0.553      0.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      2.18G      1.259      1.833      1.679         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.576      0.579      0.562      0.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      2.18G      1.219      1.701      1.625         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.813      0.734      0.797      0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      2.18G      1.251      1.609      1.656         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.735      0.685      0.757      0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      2.18G      1.175      2.106       1.81          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  4.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.766      0.699      0.695      0.353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      2.18G       1.18      2.043       1.78          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.778      0.785      0.829      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      2.18G      1.135      1.833      1.698          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50       0.67       0.78      0.781       0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      2.18G      1.057      1.748      1.695          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.679      0.824      0.794        0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      2.18G          1      1.577      1.619          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00,  6.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.874      0.862      0.859      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      2.18G     0.9595      1.518      1.586          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.862      0.881        0.9      0.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      2.18G     0.9334      1.475      1.569          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.841      0.841      0.899      0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      2.18G     0.9147      1.435      1.543          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00,  6.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.882      0.843      0.911      0.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      2.18G     0.8628      1.359      1.459          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00,  6.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.938      0.838      0.936      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      2.18G     0.8525      1.308      1.425          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.892      0.897      0.945      0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.71 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "Model summary (fused): 168 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.892      0.897      0.945      0.663\n",
      "                  weed         24         24      0.905      0.958      0.983      0.769\n",
      "              non-weed         26         26      0.879      0.836      0.906      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: invalid value encountered in less\n",
      "RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
      "âœ… YOLO Model trained and saved at yolo_weed_detector.pt\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")  \n",
    "model.train(data=\"dataset/dataset.yaml\", epochs=20, imgsz=640)\n",
    "\n",
    "# âœ… Save the trained model\n",
    "model_path = \"yolo_weed_detector.pt\"\n",
    "model.save(model_path)\n",
    "print(f\"âœ… YOLO Model trained and saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:42.879931Z",
     "iopub.status.busy": "2025-02-05T16:32:42.879639Z",
     "iopub.status.idle": "2025-02-05T16:32:42.974673Z",
     "shell.execute_reply": "2025-02-05T16:32:42.973833Z",
     "shell.execute_reply.started": "2025-02-05T16:32:42.879896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = sv.DetectionDataset.from_yolo(\n",
    "    images_directory_path=\"dataset/images/val\",\n",
    "    annotations_directory_path=\"dataset/labels/val\",\n",
    "    data_yaml_path=\"dataset/dataset.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:42.976313Z",
     "iopub.status.busy": "2025-02-05T16:32:42.976082Z",
     "iopub.status.idle": "2025-02-05T16:32:42.979771Z",
     "shell.execute_reply": "2025-02-05T16:32:42.979051Z",
     "shell.execute_reply.started": "2025-02-05T16:32:42.976292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def callback(image: np.ndarray) -> sv.Detections:\n",
    "    result = model(image)[0]  \n",
    "    return sv.Detections.from_ultralytics(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:49:02.339431Z",
     "iopub.status.busy": "2025-02-05T16:49:02.339101Z",
     "iopub.status.idle": "2025-02-05T16:49:03.765086Z",
     "shell.execute_reply": "2025-02-05T16:49:03.764433Z",
     "shell.execute_reply.started": "2025-02-05T16:49:02.339400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9430_jpeg.rf.55590fd568cfd05cb5a5f38a69781ce9.jpg: 640x640 (no detections), 9.5ms\n",
      "Speed: 3.0ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9266_jpeg.rf.190539e7297457c48131e14d0eaf9823.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9881_jpeg.rf.ee55ad20cd1edd1f366c6d2641677d66.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9532_jpeg.rf.cec78142a3ad2cbd2466c222131f846a.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_941_jpeg.rf.8d03dbb3f9b2a5aad9d050121770505a.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_8960_jpeg.rf.eccb620966057def2282a376e1bc0a53.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9152_jpeg.rf.ae839a3c5ec8f6b89ff05389895b0aa8.jpg: 640x640 (no detections), 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9132_jpeg.rf.881187a0ea138fd82cd41187f664f233.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9397_jpeg.rf.e855d959bdb985a8d189f13f16110896.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_8959_jpeg.rf.ae06f5492dd663f8a0e3c9aa344739ec.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9435_jpeg.rf.265f411a3f9f26df4433c1e1da8cd713.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9488_jpeg.rf.9032dd69a6977c2d34b8d67d9da0fe34.jpg: 640x640 (no detections), 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9143_jpeg.rf.369366d74cf68fb8264db36281a6cd38.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9586_jpeg.rf.00f91fb4178bae30704861c193fb7cef.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9161_jpeg.rf.7460b2b2f57bee0c48e02ff817724ad5.jpg: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9468_jpeg.rf.b644e3b31f9dfb1ad80f3a65e31e1a47.jpg: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9601_jpeg.rf.98980a58475f9ab84aec08679905defb.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9612_jpeg.rf.adf03073eb25e316772b8dec9b9d88d0.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9148_jpeg.rf.f906f448b054deb93b1cd1338b27b6ba.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9403_jpeg.rf.57c356d0dc56f0e64a2cc480f4028033.jpg: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9425_jpeg.rf.a565ab3aaa9f745242b2999895d5814e.jpg: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9424_jpeg.rf.f758222e980e6d4265312a3cf6ac6751.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9740_jpeg.rf.ff197f2e371323429bd9eefe5a026d0b.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9451_jpeg.rf.a0e85f372406c58574307c4397cf3657.jpg: 640x640 (no detections), 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9172_jpeg.rf.1c689e1bff67af548d03e2ba5e005e63.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_8991_jpeg.rf.e46eb3e83c49f7b83fd8b49300ff6bf6.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9220_jpeg.rf.29984de6f96d018800af07b32a2c6652.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9279_jpeg.rf.1e754c96a16dc1e519a2dc7ef32a4eb7.jpg: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9144_jpeg.rf.0a0939c3d002453a68ec987833eda345.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9208_jpeg.rf.dbf82e06e39201efc01d2b60e257e1ee.jpg: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9260_jpeg.rf.317e17b87da6fd4cd37e5071134efafa.jpg: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9707_jpeg.rf.e18f01cc8f20269a9b0a252fe489823b.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_909_jpeg.rf.38ca57a45d239c41ec78ffddb1730cc2.jpg: 640x640 (no detections), 6.7ms\n",
      "Speed: 2.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9458_jpeg.rf.3af37c99a3bf2447cd158091bd011bcd.jpg: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_935_jpeg.rf.848421eb7816778361b44f6271d2534d.jpg: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9116_jpeg.rf.812b125d18a8e7b60a363dc022354c92.jpg: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9000_jpeg.rf.7485e150688038db2f93c32b3b5d10d2.jpg: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9584_jpeg.rf.7e9cf928a6bd1413a6c4cfde2f54b435.jpg: 640x640 (no detections), 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9198_jpeg.rf.266cc70aafb5f98a6f409eed29883847.jpg: 640x640 (no detections), 6.9ms\n",
      "Speed: 2.4ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9502_jpeg.rf.bbfebc49790c0130090b6e87435e9b31.jpg: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9135_jpeg.rf.f5afbc71d5606255b3522bfc6feec990.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9183_jpeg.rf.cbd4715d486c8ef15c2ad9fa57f1989f.jpg: 640x640 (no detections), 6.7ms\n",
      "Speed: 2.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9474_jpeg.rf.34577c6abe6a2706466c111dab7520ed.jpg: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9489_jpeg.rf.28631fd4816fce6273ea3a7c3aed3794.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9810_jpeg.rf.77e8bca37c1c18f5207554e8a0f5cb32.jpg: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9527_jpeg.rf.304b5429fa5077881aae1e4775fcdc67.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9240_jpeg.rf.bc4209be17da91a36b445a77df419fdb.jpg: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9563_jpeg.rf.2d5256b218be40426425be9754ed2bab.jpg: 640x640 (no detections), 6.9ms\n",
      "Speed: 2.3ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9764_jpeg.rf.264c672fa7b490d968f13047c91b67bb.jpg: 640x640 (no detections), 7.3ms\n",
      "Speed: 2.6ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9698_jpeg.rf.2d42e15fa94fabb35f31fe8f1054fde0.jpg: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.4ms\n",
      "Speed: 2.3ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.2ms\n",
      "Speed: 2.5ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.8ms\n",
      "Speed: 2.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.5ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.4ms\n",
      "Speed: 2.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.4ms\n",
      "Speed: 2.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Final Score: 0.1711\n"
     ]
    }
   ],
   "source": [
    "def run_yolo_on_test(test_df, model):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        image_path = row[\"image\"]\n",
    "        true_label = row[\"label\"]\n",
    "\n",
    "        results = model(image_path)\n",
    "\n",
    "        if len(results[0].boxes) == 0:\n",
    "            y_pred.append(1)  # If no detection, assume it's non-weed\n",
    "        else:\n",
    "            best_idx = results[0].boxes.conf.argmax().item()\n",
    "            best_label = int(results[0].boxes.cls[best_idx].cpu().numpy())\n",
    "            y_pred.append(best_label)\n",
    "\n",
    "        y_true.append(true_label)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# âœ… Run YOLO and collect predictions\n",
    "y_true, y_pred = run_yolo_on_test(test_df, model)\n",
    "\n",
    "# âœ… Compute F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "mean_average_precision = sv.MeanAveragePrecision.benchmark(\n",
    "    dataset=dataset,\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "mAP = mean_average_precision.map50_95\n",
    "\n",
    "final_score = 0.5 * f1 + 0.5 * mAP\n",
    "print(f\"Final Score: {final_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilising unlabeled images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:44.711538Z",
     "iopub.status.busy": "2025-02-05T16:32:44.711241Z",
     "iopub.status.idle": "2025-02-05T16:32:44.762949Z",
     "shell.execute_reply": "2025-02-05T16:32:44.762262Z",
     "shell.execute_reply.started": "2025-02-05T16:32:44.711517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… YOLO Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained YOLO model\n",
    "model = YOLO(\"yolo_weed_detector.pt\")\n",
    "print(\"âœ… YOLO Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:44.763984Z",
     "iopub.status.busy": "2025-02-05T16:32:44.763754Z",
     "iopub.status.idle": "2025-02-05T16:32:44.770117Z",
     "shell.execute_reply": "2025-02-05T16:32:44.769290Z",
     "shell.execute_reply.started": "2025-02-05T16:32:44.763963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_labels(image_folder, model):\n",
    "    \"\"\"\n",
    "    Run YOLO on all images in `image_folder` and return a DataFrame with predictions.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for img_name in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"âš ï¸ Skipping {img_name}, unable to read.\")\n",
    "            continue\n",
    "\n",
    "        results = model(img_path)\n",
    "\n",
    "        if len(results[0].boxes) == 0:\n",
    "            print(f\"âš ï¸ No detections in {img_name}. Assigning default label 1 (non-weed).\")\n",
    "            data.append([img_path, 1, 0, 0, 0, 0])  # Default label for no detection\n",
    "            continue\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes.xywh.cpu().numpy()  # Convert to YOLO format (x_center, y_center, width, height)\n",
    "            labels = result.boxes.cls.cpu().numpy()  # Class labels\n",
    "\n",
    "            for box, label in zip(boxes, labels):\n",
    "                x_centre, y_centre, width, height = box\n",
    "                label = int(label)  # Convert label to integer\n",
    "                data.append([img_path, label, x_centre, y_centre, width, height])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    new_df = pd.DataFrame(data, columns=[\"image\", \"label\", \"x_centre\", \"y_centre\", \"width\", \"height\"])\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate labels for unlabeled images\n",
    "new_df = generate_labels(unlabeled_images_path, model)\n",
    "\n",
    "# Display first few rows of generated labels\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Generated Labels\", dataframe=new_df)\n",
    "\n",
    "print(\"âœ… Labels generated and stored in new_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:35:23.552965Z",
     "iopub.status.busy": "2025-02-05T16:35:23.552650Z",
     "iopub.status.idle": "2025-02-05T16:35:23.564821Z",
     "shell.execute_reply": "2025-02-05T16:35:23.563918Z",
     "shell.execute_reply.started": "2025-02-05T16:35:23.552939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>x_centre</th>\n",
       "      <th>y_centre</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>1</td>\n",
       "      <td>256.274933</td>\n",
       "      <td>260.519348</td>\n",
       "      <td>502.361267</td>\n",
       "      <td>363.015961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>1</td>\n",
       "      <td>274.809814</td>\n",
       "      <td>252.528229</td>\n",
       "      <td>284.254028</td>\n",
       "      <td>186.668259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>143.656860</td>\n",
       "      <td>439.980133</td>\n",
       "      <td>37.659081</td>\n",
       "      <td>30.384888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>93.310394</td>\n",
       "      <td>284.730347</td>\n",
       "      <td>30.548927</td>\n",
       "      <td>29.696014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>177.002319</td>\n",
       "      <td>98.791946</td>\n",
       "      <td>36.021729</td>\n",
       "      <td>31.420944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>168.335281</td>\n",
       "      <td>130.319901</td>\n",
       "      <td>302.129913</td>\n",
       "      <td>258.858490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>208.441711</td>\n",
       "      <td>241.021637</td>\n",
       "      <td>414.060364</td>\n",
       "      <td>482.043274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>294.867249</td>\n",
       "      <td>113.974419</td>\n",
       "      <td>231.541031</td>\n",
       "      <td>227.948837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>356.019257</td>\n",
       "      <td>139.217270</td>\n",
       "      <td>164.814453</td>\n",
       "      <td>277.630890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>189.094498</td>\n",
       "      <td>289.127686</td>\n",
       "      <td>370.400787</td>\n",
       "      <td>417.406433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1144 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  label    x_centre  \\\n",
       "0     /kaggle/input/weed-detection-dataset/unlabeled...      1  256.274933   \n",
       "1     /kaggle/input/weed-detection-dataset/unlabeled...      1  274.809814   \n",
       "2     /kaggle/input/weed-detection-dataset/unlabeled...      0  143.656860   \n",
       "3     /kaggle/input/weed-detection-dataset/unlabeled...      0   93.310394   \n",
       "4     /kaggle/input/weed-detection-dataset/unlabeled...      0  177.002319   \n",
       "...                                                 ...    ...         ...   \n",
       "1139  /kaggle/input/weed-detection-dataset/unlabeled...      0  168.335281   \n",
       "1140  /kaggle/input/weed-detection-dataset/unlabeled...      0  208.441711   \n",
       "1141  /kaggle/input/weed-detection-dataset/unlabeled...      0  294.867249   \n",
       "1142  /kaggle/input/weed-detection-dataset/unlabeled...      0  356.019257   \n",
       "1143  /kaggle/input/weed-detection-dataset/unlabeled...      0  189.094498   \n",
       "\n",
       "        y_centre       width      height  \n",
       "0     260.519348  502.361267  363.015961  \n",
       "1     252.528229  284.254028  186.668259  \n",
       "2     439.980133   37.659081   30.384888  \n",
       "3     284.730347   30.548927   29.696014  \n",
       "4      98.791946   36.021729   31.420944  \n",
       "...          ...         ...         ...  \n",
       "1139  130.319901  302.129913  258.858490  \n",
       "1140  241.021637  414.060364  482.043274  \n",
       "1141  113.974419  231.541031  227.948837  \n",
       "1142  139.217270  164.814453  277.630890  \n",
       "1143  289.127686  370.400787  417.406433  \n",
       "\n",
       "[1144 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:35:27.549000Z",
     "iopub.status.busy": "2025-02-05T16:35:27.548691Z",
     "iopub.status.idle": "2025-02-05T16:35:27.565585Z",
     "shell.execute_reply": "2025-02-05T16:35:27.564894Z",
     "shell.execute_reply.started": "2025-02-05T16:35:27.548975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>x_centre</th>\n",
       "      <th>y_centre</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555321</td>\n",
       "      <td>0.581240</td>\n",
       "      <td>0.981314</td>\n",
       "      <td>0.713919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595485</td>\n",
       "      <td>0.563411</td>\n",
       "      <td>0.555263</td>\n",
       "      <td>0.367108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311290</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.073563</td>\n",
       "      <td>0.059756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202194</td>\n",
       "      <td>0.635257</td>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.058401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383546</td>\n",
       "      <td>0.220413</td>\n",
       "      <td>0.070365</td>\n",
       "      <td>0.061793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.364765</td>\n",
       "      <td>0.290754</td>\n",
       "      <td>0.590182</td>\n",
       "      <td>0.509080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.451672</td>\n",
       "      <td>0.537739</td>\n",
       "      <td>0.808827</td>\n",
       "      <td>0.948002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.638947</td>\n",
       "      <td>0.254286</td>\n",
       "      <td>0.452293</td>\n",
       "      <td>0.448292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.771457</td>\n",
       "      <td>0.310605</td>\n",
       "      <td>0.321949</td>\n",
       "      <td>0.545998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409748</td>\n",
       "      <td>0.645067</td>\n",
       "      <td>0.723542</td>\n",
       "      <td>0.820885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1144 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  label  x_centre  \\\n",
       "0     /kaggle/input/weed-detection-dataset/unlabeled...      1  0.555321   \n",
       "1     /kaggle/input/weed-detection-dataset/unlabeled...      1  0.595485   \n",
       "2     /kaggle/input/weed-detection-dataset/unlabeled...      0  0.311290   \n",
       "3     /kaggle/input/weed-detection-dataset/unlabeled...      0  0.202194   \n",
       "4     /kaggle/input/weed-detection-dataset/unlabeled...      0  0.383546   \n",
       "...                                                 ...    ...       ...   \n",
       "1139  /kaggle/input/weed-detection-dataset/unlabeled...      0  0.364765   \n",
       "1140  /kaggle/input/weed-detection-dataset/unlabeled...      0  0.451672   \n",
       "1141  /kaggle/input/weed-detection-dataset/unlabeled...      0  0.638947   \n",
       "1142  /kaggle/input/weed-detection-dataset/unlabeled...      0  0.771457   \n",
       "1143  /kaggle/input/weed-detection-dataset/unlabeled...      0  0.409748   \n",
       "\n",
       "      y_centre     width    height  \n",
       "0     0.581240  0.981314  0.713919  \n",
       "1     0.563411  0.555263  0.367108  \n",
       "2     0.981632  0.073563  0.059756  \n",
       "3     0.635257  0.059674  0.058401  \n",
       "4     0.220413  0.070365  0.061793  \n",
       "...        ...       ...       ...  \n",
       "1139  0.290754  0.590182  0.509080  \n",
       "1140  0.537739  0.808827  0.948002  \n",
       "1141  0.254286  0.452293  0.448292  \n",
       "1142  0.310605  0.321949  0.545998  \n",
       "1143  0.645067  0.723542  0.820885  \n",
       "\n",
       "[1144 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing numerical columns to ensure values are between 0 and 1\n",
    "numerical_columns = [\"x_centre\", \"y_centre\", \"width\", \"height\"]\n",
    "\n",
    "# Applying min-max normalization\n",
    "new_df[numerical_columns] = new_df[numerical_columns].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:37:46.094175Z",
     "iopub.status.busy": "2025-02-05T16:37:46.093832Z",
     "iopub.status.idle": "2025-02-05T16:37:46.107293Z",
     "shell.execute_reply": "2025-02-05T16:37:46.106567Z",
     "shell.execute_reply.started": "2025-02-05T16:37:46.094151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>x_centre</th>\n",
       "      <th>y_centre</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555321</td>\n",
       "      <td>0.581240</td>\n",
       "      <td>0.981314</td>\n",
       "      <td>0.713919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.595485</td>\n",
       "      <td>0.563411</td>\n",
       "      <td>0.555263</td>\n",
       "      <td>0.367108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311290</td>\n",
       "      <td>0.981632</td>\n",
       "      <td>0.073563</td>\n",
       "      <td>0.059756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202194</td>\n",
       "      <td>0.635257</td>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.058401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/unlabeled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383546</td>\n",
       "      <td>0.220413</td>\n",
       "      <td>0.070365</td>\n",
       "      <td>0.061793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.337891</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.574219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.499023</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>0.748047</td>\n",
       "      <td>0.603516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.673828</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.371094</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524414</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>/kaggle/input/weed-detection-dataset/labeled/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477539</td>\n",
       "      <td>0.533203</td>\n",
       "      <td>0.451172</td>\n",
       "      <td>0.738281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1344 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  label  x_centre  \\\n",
       "0     /kaggle/input/weed-detection-dataset/unlabeled...      1  0.555321   \n",
       "1     /kaggle/input/weed-detection-dataset/unlabeled...      1  0.595485   \n",
       "2     /kaggle/input/weed-detection-dataset/unlabeled...      0  0.311290   \n",
       "3     /kaggle/input/weed-detection-dataset/unlabeled...      0  0.202194   \n",
       "4     /kaggle/input/weed-detection-dataset/unlabeled...      0  0.383546   \n",
       "...                                                 ...    ...       ...   \n",
       "1339  /kaggle/input/weed-detection-dataset/labeled/i...      0  0.335938   \n",
       "1340  /kaggle/input/weed-detection-dataset/labeled/i...      1  0.499023   \n",
       "1341  /kaggle/input/weed-detection-dataset/labeled/i...      1  0.673828   \n",
       "1342  /kaggle/input/weed-detection-dataset/labeled/i...      0  0.524414   \n",
       "1343  /kaggle/input/weed-detection-dataset/labeled/i...      1  0.477539   \n",
       "\n",
       "      y_centre     width    height  \n",
       "0     0.581240  0.981314  0.713919  \n",
       "1     0.563411  0.555263  0.367108  \n",
       "2     0.981632  0.073563  0.059756  \n",
       "3     0.635257  0.059674  0.058401  \n",
       "4     0.220413  0.070365  0.061793  \n",
       "...        ...       ...       ...  \n",
       "1339  0.337891  0.632812  0.574219  \n",
       "1340  0.348633  0.748047  0.603516  \n",
       "1341  0.279297  0.371094  0.453125  \n",
       "1342  0.531250  0.767578  0.855469  \n",
       "1343  0.533203  0.451172  0.738281  \n",
       "\n",
       "[1344 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comb_df = pd.concat([new_df, df], ignore_index=True)\n",
    "comb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:39:33.628729Z",
     "iopub.status.busy": "2025-02-05T16:39:33.628429Z",
     "iopub.status.idle": "2025-02-05T16:39:33.634275Z",
     "shell.execute_reply": "2025-02-05T16:39:33.633385Z",
     "shell.execute_reply.started": "2025-02-05T16:39:33.628707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset folders created.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset\"\n",
    "os.makedirs(f\"{dataset_path}/images/train\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/images/val\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/labels/train\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/labels/val\", exist_ok=True)\n",
    "\n",
    "print(\"âœ… Dataset folders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:39:51.618992Z",
     "iopub.status.busy": "2025-02-05T16:39:51.618656Z",
     "iopub.status.idle": "2025-02-05T16:39:53.716173Z",
     "shell.execute_reply": "2025-02-05T16:39:53.715455Z",
     "shell.execute_reply.started": "2025-02-05T16:39:51.618963Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Images copied to dataset folders.\n"
     ]
    }
   ],
   "source": [
    "def move_images(df, img_folder):\n",
    "    for img_path in df[\"image\"]:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        new_path = os.path.join(img_folder, img_name)\n",
    "        shutil.copy(img_path, new_path)\n",
    "\n",
    "# âœ… Move training images\n",
    "move_images(comb_df, \"dataset/images/train\")\n",
    "\n",
    "# âœ… Move validation images (from test_df)\n",
    "move_images(test_df, \"dataset/images/val\")\n",
    "\n",
    "print(\"âœ… Images copied to dataset folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:40:00.152807Z",
     "iopub.status.busy": "2025-02-05T16:40:00.152525Z",
     "iopub.status.idle": "2025-02-05T16:40:00.211260Z",
     "shell.execute_reply": "2025-02-05T16:40:00.210437Z",
     "shell.execute_reply.started": "2025-02-05T16:40:00.152784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… YOLO labels saved.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_yolo(df, label_folder):\n",
    "    for _, row in df.iterrows():\n",
    "        img_name = os.path.basename(row[\"image\"]).replace(\".jpg\", \".txt\")  \n",
    "        label_file = os.path.join(label_folder, img_name)\n",
    "\n",
    "        with open(label_file, \"w\") as f:\n",
    "            f.write(f\"{row['label']} {row['x_centre']} {row['y_centre']} {row['width']} {row['height']}\\n\")\n",
    "\n",
    "# âœ… Create labels for training and validation\n",
    "convert_to_yolo(df, \"dataset/labels/train\")\n",
    "convert_to_yolo(test_df, \"dataset/labels/val\")\n",
    "\n",
    "print(\"âœ… YOLO labels saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:40:08.134014Z",
     "iopub.status.busy": "2025-02-05T16:40:08.133647Z",
     "iopub.status.idle": "2025-02-05T16:40:08.139414Z",
     "shell.execute_reply": "2025-02-05T16:40:08.138657Z",
     "shell.execute_reply.started": "2025-02-05T16:40:08.133983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dataset.yaml created.\n"
     ]
    }
   ],
   "source": [
    "dataset_abs_path = os.path.abspath(\"dataset\")\n",
    "\n",
    "yaml_content = f\"\"\"\n",
    "path: {dataset_abs_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: 2\n",
    "names: [\"weed\", \"non-weed\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dataset/dataset.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"âœ… dataset.yaml created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training on labeled+unlabeled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:40:15.807906Z",
     "iopub.status.busy": "2025-02-05T16:40:15.807564Z",
     "iopub.status.idle": "2025-02-05T16:44:51.168026Z",
     "shell.execute_reply": "2025-02-05T16:44:51.167094Z",
     "shell.execute_reply.started": "2025-02-05T16:40:15.807846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.71 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset/dataset.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/labels/train... 200 images, 1000 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 1770.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/labels/val.cache... 50 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      2.24G      1.499      12.51      1.884          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.695      0.208      0.275     0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20       2.2G       1.65       6.55      1.995          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  6.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50    0.00279      0.804     0.0428     0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      2.27G      1.782      4.734      2.149          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  6.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0147      0.288     0.0812      0.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      2.27G      1.858      4.356      2.168          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50    0.00242      0.724     0.0341    0.00987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      2.27G      1.957      4.383      2.241          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0123      0.883     0.0859     0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      2.18G      1.909      3.751      2.183          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  6.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0143      0.489     0.0669     0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      2.18G      1.931      3.451      2.145          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  6.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0254      0.787      0.227     0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      2.18G       1.76      3.137      2.054          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  6.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.143      0.748      0.375      0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      2.18G       1.71      3.027      2.003          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0481      0.941       0.53      0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      2.18G      1.714      3.209      1.985          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.162      0.881       0.51      0.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      2.18G        1.5      3.619      2.081          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  5.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50     0.0281      0.804      0.382      0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      2.18G      1.394      3.125      2.018          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.461      0.713      0.574       0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      2.18G      1.475      3.108      2.082          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.111      0.921      0.679      0.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      2.18G      1.389      2.708       2.05          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.701      0.679      0.794      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      2.18G      1.329      2.861      1.933          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.139       0.96      0.832      0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      2.18G       1.27      2.775      1.816          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.314      0.883      0.804      0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      2.18G      1.276      2.587      1.811          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.787      0.739      0.834      0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      2.18G      1.152      2.519      1.706          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.783      0.839      0.852      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      2.18G      1.255      2.572      1.817          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.825      0.847      0.888      0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      2.18G      1.185      2.539      1.759          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:11<00:00,  6.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.824      0.842        0.9       0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.074 hours.\n",
      "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train3/weights/best.pt...\n",
      "Ultralytics 8.3.71 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "Model summary (fused): 168 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         50      0.818      0.847      0.899       0.59\n",
      "                  weed         24         24      0.842      0.886      0.955      0.645\n",
      "              non-weed         26         26      0.795      0.808      0.843      0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: invalid value encountered in less\n",
      "RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
      "âœ… YOLO Model trained and saved at yolo_weed_detector.pt\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")  \n",
    "model.train(data=\"dataset/dataset.yaml\", epochs=20, imgsz=640)\n",
    "\n",
    "# âœ… Save the trained model\n",
    "model_path = \"yolo_weed_detector.pt\"\n",
    "model.save(model_path)\n",
    "print(f\"âœ… YOLO Model trained and saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:45:11.099019Z",
     "iopub.status.busy": "2025-02-05T16:45:11.098650Z",
     "iopub.status.idle": "2025-02-05T16:45:11.193252Z",
     "shell.execute_reply": "2025-02-05T16:45:11.192614Z",
     "shell.execute_reply.started": "2025-02-05T16:45:11.098984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = sv.DetectionDataset.from_yolo(\n",
    "    images_directory_path=\"dataset/images/val\",\n",
    "    annotations_directory_path=\"dataset/labels/val\",\n",
    "    data_yaml_path=\"dataset/dataset.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:45:25.023422Z",
     "iopub.status.busy": "2025-02-05T16:45:25.023127Z",
     "iopub.status.idle": "2025-02-05T16:45:25.027505Z",
     "shell.execute_reply": "2025-02-05T16:45:25.026713Z",
     "shell.execute_reply.started": "2025-02-05T16:45:25.023396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def callback(image: np.ndarray) -> sv.Detections:\n",
    "    result = model(image)[0]  \n",
    "    return sv.Detections.from_ultralytics(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:43.870636Z",
     "iopub.status.busy": "2025-02-05T16:32:43.870298Z",
     "iopub.status.idle": "2025-02-05T16:32:44.688443Z",
     "shell.execute_reply": "2025-02-05T16:32:44.687634Z",
     "shell.execute_reply.started": "2025-02-05T16:32:43.870599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9430_jpeg.rf.55590fd568cfd05cb5a5f38a69781ce9.jpg: 640x640 1 weed, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9266_jpeg.rf.190539e7297457c48131e14d0eaf9823.jpg: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9881_jpeg.rf.ee55ad20cd1edd1f366c6d2641677d66.jpg: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9532_jpeg.rf.cec78142a3ad2cbd2466c222131f846a.jpg: 640x640 1 non-weed, 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_941_jpeg.rf.8d03dbb3f9b2a5aad9d050121770505a.jpg: 640x640 1 non-weed, 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_8960_jpeg.rf.eccb620966057def2282a376e1bc0a53.jpg: 640x640 1 non-weed, 6.4ms\n",
      "Speed: 2.2ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9152_jpeg.rf.ae839a3c5ec8f6b89ff05389895b0aa8.jpg: 640x640 1 weed, 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9132_jpeg.rf.881187a0ea138fd82cd41187f664f233.jpg: 640x640 2 weeds, 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9397_jpeg.rf.e855d959bdb985a8d189f13f16110896.jpg: 640x640 2 weeds, 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_8959_jpeg.rf.ae06f5492dd663f8a0e3c9aa344739ec.jpg: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9435_jpeg.rf.265f411a3f9f26df4433c1e1da8cd713.jpg: 640x640 1 non-weed, 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9488_jpeg.rf.9032dd69a6977c2d34b8d67d9da0fe34.jpg: 640x640 1 weed, 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9143_jpeg.rf.369366d74cf68fb8264db36281a6cd38.jpg: 640x640 1 non-weed, 6.4ms\n",
      "Speed: 2.2ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9586_jpeg.rf.00f91fb4178bae30704861c193fb7cef.jpg: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9161_jpeg.rf.7460b2b2f57bee0c48e02ff817724ad5.jpg: 640x640 1 non-weed, 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9468_jpeg.rf.b644e3b31f9dfb1ad80f3a65e31e1a47.jpg: 640x640 1 non-weed, 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9601_jpeg.rf.98980a58475f9ab84aec08679905defb.jpg: 640x640 1 weed, 6.4ms\n",
      "Speed: 2.3ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9612_jpeg.rf.adf03073eb25e316772b8dec9b9d88d0.jpg: 640x640 1 non-weed, 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9148_jpeg.rf.f906f448b054deb93b1cd1338b27b6ba.jpg: 640x640 3 non-weeds, 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9403_jpeg.rf.57c356d0dc56f0e64a2cc480f4028033.jpg: 640x640 1 non-weed, 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9425_jpeg.rf.a565ab3aaa9f745242b2999895d5814e.jpg: 640x640 2 non-weeds, 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9424_jpeg.rf.f758222e980e6d4265312a3cf6ac6751.jpg: 640x640 2 weeds, 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9740_jpeg.rf.ff197f2e371323429bd9eefe5a026d0b.jpg: 640x640 1 weed, 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9451_jpeg.rf.a0e85f372406c58574307c4397cf3657.jpg: 640x640 2 weeds, 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9172_jpeg.rf.1c689e1bff67af548d03e2ba5e005e63.jpg: 640x640 1 weed, 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_8991_jpeg.rf.e46eb3e83c49f7b83fd8b49300ff6bf6.jpg: 640x640 2 weeds, 6.4ms\n",
      "Speed: 2.2ms preprocess, 6.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9220_jpeg.rf.29984de6f96d018800af07b32a2c6652.jpg: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9279_jpeg.rf.1e754c96a16dc1e519a2dc7ef32a4eb7.jpg: 640x640 1 non-weed, 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9144_jpeg.rf.0a0939c3d002453a68ec987833eda345.jpg: 640x640 1 weed, 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9208_jpeg.rf.dbf82e06e39201efc01d2b60e257e1ee.jpg: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9260_jpeg.rf.317e17b87da6fd4cd37e5071134efafa.jpg: 640x640 1 non-weed, 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9707_jpeg.rf.e18f01cc8f20269a9b0a252fe489823b.jpg: 640x640 1 weed, 6.5ms\n",
      "Speed: 2.3ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_909_jpeg.rf.38ca57a45d239c41ec78ffddb1730cc2.jpg: 640x640 1 non-weed, 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9458_jpeg.rf.3af37c99a3bf2447cd158091bd011bcd.jpg: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_935_jpeg.rf.848421eb7816778361b44f6271d2534d.jpg: 640x640 1 weed, 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9116_jpeg.rf.812b125d18a8e7b60a363dc022354c92.jpg: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9000_jpeg.rf.7485e150688038db2f93c32b3b5d10d2.jpg: 640x640 1 weed, 6.8ms\n",
      "Speed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9584_jpeg.rf.7e9cf928a6bd1413a6c4cfde2f54b435.jpg: 640x640 1 weed, 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9198_jpeg.rf.266cc70aafb5f98a6f409eed29883847.jpg: 640x640 1 weed, 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9502_jpeg.rf.bbfebc49790c0130090b6e87435e9b31.jpg: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9135_jpeg.rf.f5afbc71d5606255b3522bfc6feec990.jpg: 640x640 2 non-weeds, 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9183_jpeg.rf.cbd4715d486c8ef15c2ad9fa57f1989f.jpg: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9474_jpeg.rf.34577c6abe6a2706466c111dab7520ed.jpg: 640x640 1 weed, 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9489_jpeg.rf.28631fd4816fce6273ea3a7c3aed3794.jpg: 640x640 1 weed, 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9810_jpeg.rf.77e8bca37c1c18f5207554e8a0f5cb32.jpg: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9527_jpeg.rf.304b5429fa5077881aae1e4775fcdc67.jpg: 640x640 1 non-weed, 6.4ms\n",
      "Speed: 2.2ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9240_jpeg.rf.bc4209be17da91a36b445a77df419fdb.jpg: 640x640 1 non-weed, 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9563_jpeg.rf.2d5256b218be40426425be9754ed2bab.jpg: 640x640 1 weed, 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9764_jpeg.rf.264c672fa7b490d968f13047c91b67bb.jpg: 640x640 1 non-weed, 6.6ms\n",
      "Speed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /kaggle/input/weed-detection-dataset/test/images/agri_0_9698_jpeg.rf.2d42e15fa94fabb35f31fe8f1054fde0.jpg: 640x640 1 weed, 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "âœ… F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def run_yolo_on_test(test_df, model):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        image_path = row[\"image\"]\n",
    "        true_label = row[\"label\"]\n",
    "\n",
    "        results = model(image_path)\n",
    "\n",
    "        if len(results[0].boxes) == 0:\n",
    "            y_pred.append(1)  # If no detection, assume it's non-weed\n",
    "        else:\n",
    "            best_idx = results[0].boxes.conf.argmax().item()\n",
    "            best_label = int(results[0].boxes.cls[best_idx].cpu().numpy())\n",
    "            y_pred.append(best_label)\n",
    "\n",
    "        y_true.append(true_label)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# âœ… Run YOLO and collect predictions\n",
    "y_true, y_pred = run_yolo_on_test(test_df, model)\n",
    "\n",
    "# âœ… Compute F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:42.981106Z",
     "iopub.status.busy": "2025-02-05T16:32:42.980911Z",
     "iopub.status.idle": "2025-02-05T16:32:43.869213Z",
     "shell.execute_reply": "2025-02-05T16:32:43.868557Z",
     "shell.execute_reply.started": "2025-02-05T16:32:42.981087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 weeds, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.4ms\n",
      "Speed: 2.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 non-weeds, 7.4ms\n",
      "Speed: 2.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 non-weeds, 7.4ms\n",
      "Speed: 2.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 non-weeds, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 7.3ms\n",
      "Speed: 2.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.3ms\n",
      "Speed: 2.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 7.3ms\n",
      "Speed: 2.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 weeds, 6.7ms\n",
      "Speed: 2.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 weeds, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.2ms\n",
      "Speed: 2.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 7.0ms\n",
      "Speed: 2.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.2ms\n",
      "Speed: 2.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 weeds, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.4ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.2ms\n",
      "Speed: 2.3ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.3ms\n",
      "Speed: 2.3ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.9ms\n",
      "Speed: 2.4ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.6ms\n",
      "Speed: 2.3ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.4ms\n",
      "Speed: 2.5ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.7ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 weeds, 6.1ms\n",
      "Speed: 2.5ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 6.5ms\n",
      "Speed: 2.4ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 weed, 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 non-weed, 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "âœ… mAP@[0.5:0.95]: 0.6912\n"
     ]
    }
   ],
   "source": [
    "mean_average_precision = sv.MeanAveragePrecision.benchmark(\n",
    "    dataset=dataset,\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "mAP = mean_average_precision.map50_95\n",
    "print(f\"âœ… mAP@[0.5:0.95]: {mAP:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:32:44.689789Z",
     "iopub.status.busy": "2025-02-05T16:32:44.689493Z",
     "iopub.status.idle": "2025-02-05T16:32:44.693944Z",
     "shell.execute_reply": "2025-02-05T16:32:44.693309Z",
     "shell.execute_reply.started": "2025-02-05T16:32:44.689755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 0.8456\n"
     ]
    }
   ],
   "source": [
    "final_score = 0.5 * f1 + 0.5 * mAP\n",
    "print(f\"Final Score: {final_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our performance has improved by a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6558789,
     "sourceId": 10596503,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
